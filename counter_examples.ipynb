{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForMaskedLM(\n",
       "  (activation): GELUActivation()\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (vocab_projector): Linear(in_features=768, out_features=30522, bias=True)\n",
       "  (mlm_loss_fct): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/masked_s_gold_BUG.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among them was the president [MASK] .\n",
      "[MASK]: 'himself'  | weights: 0.048793725669384\n",
      "[MASK]: 'marcos'  | weights: 0.012270461767911911\n",
      "[MASK]: 'vargas'  | weights: 0.010119077749550343\n",
      "[MASK]: 'diaz'  | weights: 0.009520509280264378\n",
      "[MASK]: 'fernandez'  | weights: 0.00824071280658245\n",
      "Results In the pre - COVID era , an average aesthetic surgeon was finely balancing [MASK] profession , personal lifestyle , learning , and recreation .\n",
      "[MASK]: 'in'  | weights: 0.13020457327365875\n",
      "[MASK]: 'aesthetic'  | weights: 0.08425050973892212\n",
      "[MASK]: 'for'  | weights: 0.07207341492176056\n",
      "[MASK]: ':'  | weights: 0.06453636288642883\n",
      "[MASK]: 'through'  | weights: 0.06102780997753143\n",
      "Peter ( B23 ) is a musician and music editor who has been using [MASK] MacBook Pro and a specialist application ( MainStage ) to emulate \" the sounds of pro keyboards like Roland RD pianos and synths when playing live \" .\n",
      "[MASK]: 'apple'  | weights: 0.29093122482299805\n",
      "[MASK]: 'a'  | weights: 0.18409289419651031\n",
      "[MASK]: 'the'  | weights: 0.0926438495516777\n",
      "[MASK]: 'microsoft'  | weights: 0.04886259511113167\n",
      "[MASK]: 'his'  | weights: 0.026158662512898445\n",
      "We use genetic data from this child and [MASK] family to illustrate how comprehensive genetic profiling may also assist in the characterisation of 16p11.2 microdeletion syndrome .\n",
      "[MASK]: 'their'  | weights: 0.16210685670375824\n",
      "[MASK]: 'her'  | weights: 0.12177655100822449\n",
      "[MASK]: 'his'  | weights: 0.1156599149107933\n",
      "[MASK]: 'its'  | weights: 0.08606614172458649\n",
      "[MASK]: 'parent'  | weights: 0.07100826501846313\n",
      "These two might not have any social contact in normal situation , however , when node j ( the student ) is sick , [MASK] can possibly pass infection to node i ( the nurse ) ; and this is the contact important for epidemic modeling purpose .\n",
      "[MASK]: 'they'  | weights: 0.15716181695461273\n",
      "[MASK]: 'one'  | weights: 0.11617842316627502\n",
      "[MASK]: 'infection'  | weights: 0.09569600224494934\n",
      "[MASK]: 'it'  | weights: 0.05150054395198822\n",
      "[MASK]: 'he'  | weights: 0.03846658766269684\n",
      "The author declares that [MASK] has no conflict of interest\n",
      "[MASK]: 'it'  | weights: 0.04340707138180733\n",
      "[MASK]: 'copyright'  | weights: 0.02625502087175846\n",
      "[MASK]: 'religion'  | weights: 0.010418446734547615\n",
      "[MASK]: 'marriage'  | weights: 0.009022357873618603\n",
      "[MASK]: 'equity'  | weights: 0.008391612209379673\n",
      "Child and [MASK] gifted wife .\n",
      "[MASK]: 'exceptionally'  | weights: 0.09905804693698883\n",
      "[MASK]: 'socially'  | weights: 0.06340409815311432\n",
      "[MASK]: 'mentally'  | weights: 0.06287230551242828\n",
      "[MASK]: 'emotionally'  | weights: 0.0623503141105175\n",
      "[MASK]: 'a'  | weights: 0.05290209501981735\n",
      "( d. 1837 ) , a New England physician , who moved to New York and established [MASK] as bookseller .\n",
      "[MASK]: 'himself'  | weights: 0.9671034812927246\n",
      "[MASK]: 'herself'  | weights: 0.018089767545461655\n",
      "[MASK]: 'itself'  | weights: 0.006455391179770231\n",
      "[MASK]: 'business'  | weights: 0.0029982361011207104\n",
      "[MASK]: 'themselves'  | weights: 0.001796460011973977\n",
      "Soon after , a courier informed me that Colonel Chilton wished to see me in front , on the Darbytown road , and that [MASK] was sent to conduct me to him .\n",
      "[MASK]: 'he'  | weights: 0.1330122947692871\n",
      "[MASK]: 'someone'  | weights: 0.05355449020862579\n",
      "[MASK]: 'messenger'  | weights: 0.021161869168281555\n",
      "[MASK]: 'i'  | weights: 0.01611342467367649\n",
      "[MASK]: 'colonel'  | weights: 0.012845510616898537\n",
      "The driver , however , was a boorish fellow , and paid no heed to him , but drove [MASK] team along .\n",
      "[MASK]: 'his'  | weights: 0.6097750067710876\n",
      "[MASK]: 'the'  | weights: 0.19280365109443665\n",
      "[MASK]: 'neither'  | weights: 0.13020801544189453\n",
      "[MASK]: 'another'  | weights: 0.013612071052193642\n",
      "[MASK]: 'their'  | weights: 0.011603670194745064\n",
      "15 . What else was there in which a man of high birth and a distinguished lawyer would sooner admit [MASK] inferiority ?\n",
      "[MASK]: 'such'  | weights: 0.08078356087207794\n",
      "[MASK]: 'any'  | weights: 0.04681118205189705\n",
      "[MASK]: 'relative'  | weights: 0.03652430325746536\n",
      "[MASK]: 'his'  | weights: 0.029587777331471443\n",
      "[MASK]: 'moral'  | weights: 0.027118327096104622\n",
      "The New Zealand child and [MASK] family \" , was published in 1970 .\n",
      "[MASK]: 'his'  | weights: 0.1355346143245697\n",
      "[MASK]: 'adolescent'  | weights: 0.09478644281625748\n",
      "[MASK]: 'foster'  | weights: 0.06482400000095367\n",
      "[MASK]: 'her'  | weights: 0.045772846788167953\n",
      "[MASK]: 'the'  | weights: 0.044314414262771606\n",
      "Former lead guitarist Roger Johnson currently works with [MASK] wife in publishing .\n",
      "[MASK]: 'his'  | weights: 0.9753604531288147\n",
      "[MASK]: 'her'  | weights: 0.016861477866768837\n",
      "[MASK]: 'their'  | weights: 0.0012722400715574622\n",
      "[MASK]: 'another'  | weights: 0.0008800061186775565\n",
      "[MASK]: 'a'  | weights: 0.0006881873705424368\n",
      "For [MASK] work with youth soccer , head coach Kerry Shubert was named 2008 \"\n",
      "[MASK]: 'his'  | weights: 0.3379393219947815\n",
      "[MASK]: 'outstanding'  | weights: 0.26218560338020325\n",
      "[MASK]: 'exemplary'  | weights: 0.07932741194963455\n",
      "[MASK]: 'her'  | weights: 0.02996511571109295\n",
      "[MASK]: 'excellent'  | weights: 0.027733560651540756\n",
      "On 22 March , a pilot of the Free Libyan Air Force , Colonel Fakhri Alsalabi , flew [MASK] jet into Bab Al Azizia in an apparent suicide mission , causing extensive damage and leading to rumors of Khamis Gaddafi 's death , who later was proven to have survived unharmed .\n",
      "[MASK]: 'a'  | weights: 0.46800121665000916\n",
      "[MASK]: 'his'  | weights: 0.2545264661312103\n",
      "[MASK]: 'another'  | weights: 0.13155551254749298\n",
      "[MASK]: 'the'  | weights: 0.05003131553530693\n",
      "[MASK]: 'their'  | weights: 0.024780811741948128\n",
      "Every child and every man had [MASK] utmost to contribute to survival .\n",
      "[MASK]: 'the'  | weights: 0.5962794423103333\n",
      "[MASK]: 'their'  | weights: 0.14009587466716766\n",
      "[MASK]: 'every'  | weights: 0.07851647585630417\n",
      "[MASK]: 'a'  | weights: 0.018674559891223907\n",
      "[MASK]: 'his'  | weights: 0.016234613955020905\n",
      "There is some evidence that these two gods were considered aspects of a single being , as when a singer in the asks where [MASK] can go given that \" \" ( \" they , God , stand double \" ) .\n",
      "[MASK]: 'they'  | weights: 0.36668604612350464\n",
      "[MASK]: 'angels'  | weights: 0.05598526820540428\n",
      "[MASK]: 'one'  | weights: 0.05556771159172058\n",
      "[MASK]: 'you'  | weights: 0.048403408378362656\n",
      "[MASK]: 'we'  | weights: 0.047398943454027176\n",
      "Cheers - but please when you are quoting a post , be sure not to imply that the editor wrote what [MASK] has quoted from a reliable source !\n",
      "[MASK]: 'he'  | weights: 0.13214290142059326\n",
      "[MASK]: 'someone'  | weights: 0.07657627761363983\n",
      "[MASK]: 'somebody'  | weights: 0.05112990736961365\n",
      "[MASK]: 'she'  | weights: 0.04605025425553322\n",
      "[MASK]: 'it'  | weights: 0.03288966044783592\n",
      "Teachers that select appropriate literature for their classroom needs may provide a child with a \" character in a story to help the child understand [MASK] Classroom story time and a guided discussion allows students to \" become aware of problems of other children and develop empathy \" .\n",
      "[MASK]: 'appropriate'  | weights: 0.13893814384937286\n",
      "[MASK]: 'about'  | weights: 0.11358330398797989\n",
      "[MASK]: 'proper'  | weights: 0.07408948987722397\n",
      "[MASK]: 'their'  | weights: 0.05125780403614044\n",
      "[MASK]: 'during'  | weights: 0.04430535063147545\n",
      "One \" matai \" ( chief ) was expelled from [MASK] village for suspicion of procuring prostitutes for the Americans .\n",
      "[MASK]: 'the'  | weights: 0.7469194531440735\n",
      "[MASK]: 'this'  | weights: 0.06767988204956055\n",
      "[MASK]: 'their'  | weights: 0.0188435148447752\n",
      "[MASK]: 'his'  | weights: 0.017057839781045914\n",
      "[MASK]: 'a'  | weights: 0.015283893793821335\n"
     ]
    }
   ],
   "source": [
    "def predict_masked_sent(text, top_k=5):\n",
    "    # Tokenize input\n",
    "    text = \"[CLS] %s [SEP]\"%text\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    masked_index = tokenized_text.index(\"[MASK]\")\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    # tokens_tensor = tokens_tensor.to('cuda')    # if you have gpu\n",
    "\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "\n",
    "    probs = torch.nn.functional.softmax(predictions[0, masked_index], dim=-1)\n",
    "    top_k_weights, top_k_indices = torch.topk(probs, top_k, sorted=True)\n",
    "\n",
    "    for i, pred_idx in enumerate(top_k_indices):\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([pred_idx])[0]\n",
    "        token_weight = top_k_weights[i]\n",
    "        print(\"[MASK]: '%s'\"%predicted_token, \" | weights:\", float(token_weight))\n",
    "\n",
    "for i in df.head(20)[\"text\"].values:\n",
    "    print(i)\n",
    "    predict_masked_sent(i, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224u",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
